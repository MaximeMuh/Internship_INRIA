## Introduction

Ce dÃ©pÃ´t rassemble le code et les notebooks rÃ©alisÃ©s durant mon **stage de recherche Ã  lâ€™INRIA Paris (Ã©quipe ARGO)**, effectuÃ© du **17/06/2024 au 04/09/2024** sous la supervision de **Marc Lelarge**. Lâ€™objectif gÃ©nÃ©ral du stage Ã©tait dâ€™explorer, de maniÃ¨re progressive et expÃ©rimentale, les **liens entre lâ€™assimilation de donnÃ©es** (pour des systÃ¨mes dynamiques physiques) et les **modÃ¨les gÃ©nÃ©ratifs de diffusion** (diffusion probabiliste, score-based, interpolants stochastiques), puis dâ€™en **Ã©valuer la pertinence sur des donnÃ©es structurÃ©es** (images dâ€™arbres couvrants uniformes â€” UST â€” et **graphes**).

Le travail est organisÃ© en Ã©tapes complÃ©mentaires :
1) **Assimilation de donnÃ©es** : implÃ©mentations de mÃ©thodes classiques (BLUE, 3D-VAR) et dÃ©monstrations sur des systÃ¨mes jouets (ex. pendule simple, attracteur de Lorenz) pour poser le cadre et les mÃ©triques.
2) **ModÃ¨les de diffusion (DDPM)** : mise en pratique sur des images (MNIST, CIFAR-10) afin dâ€™acquÃ©rir un socle opÃ©rationnel pour le dÃ©bruitage/gÃ©nÃ©ration et le conditionnement.
3) **Interpolants stochastiques (SDE/ODE)** : expÃ©rimentation du cadre unifiant (flows/diffusions) dâ€™**Albergo & Vanden-Eijnden** â€” dâ€™abord en **2D** (passage contrÃ´lÃ© entre deux distributions), puis sur **donnÃ©es visuelles** (ex. *Oxford Flowers*).
4) **Application Ã  lâ€™â€œassimilationâ€ dâ€™UST par lâ€™image** : **inpainting conditionnel** dâ€™arbres couvrants uniformes (UST) avec **resampling** pour amÃ©liorer la cohÃ©rence locale des rÃ©gions complÃ©tÃ©es.
5) **Diffusion sur graphes (UST)** : tentative de gÃ©nÃ©ration **discrÃ¨te** sur matrices dâ€™adjacence (inspirÃ©e des approches â€œdiscrete diffusionâ€), avec **contraintes topologiques** (connectivitÃ©, acyclicitÃ©, nombre dâ€™arÃªtes) via la fonction de perte et un **post-processing** vers des UST.

### Ce que contient le dÃ©pÃ´t (grandes lignes)

- **Assimilation (BLUE, 3D-VAR)** : notebooks pÃ©dagogiques et reproductibles avec visualisations des trajectoires assimilÃ©es.
- **Diffusion (DDPM)** : notebook dâ€™entraÃ®nement/Ã©chantillonnage sur images (dÃ©mos MNIST / CIFAR-10).
- **Interpolants stochastiques** : deux notebooks (cas **2D** et **images**) illustrant SDE/ODE et la rÃ©solution associÃ©e.
- **Inpainting UST (par lâ€™image)** : script de gÃ©nÃ©ration de masques et **complÃ©tion par diffusion** (inpainting conditionnel avec resampling).
- **Diffusion sur graphes (UST)** : notebook de diffusion **discrÃ¨te** sur **matrices dâ€™adjacence** (gÃ©nÃ©ration â†’ contraintes â†’ post-traitement en UST).
- **DonnÃ©es utilitaires** : gÃ©nÃ©ration dâ€™UST (algorithme de Wilson), jeux dâ€™essai et sorties visuelles (selon disponibilitÃ© locale).

> Le **rapport de stage (PDF)** associÃ© â€” qui prÃ©sente le contexte, les choix mÃ©thodologiques, les rÃ©sultats et la discussion â€” est inclus dans le dÃ©pÃ´t et sert de rÃ©fÃ©rence scientifique Ã  ce code.


## 1. Assimilation de donnÃ©es â€” `BLUE_3DVAR.ipynb`

Ce notebook introduit les **principes fondamentaux de lâ€™assimilation de donnÃ©es**, tels quâ€™utilisÃ©s dans les sciences physiques (mÃ©tÃ©orologie, ocÃ©anographie, dynamique des systÃ¨mes).  
Lâ€™objectif est de comprendre comment combiner des **prÃ©visions issues dâ€™un modÃ¨le** et des **observations bruitÃ©es** afin dâ€™obtenir un **Ã©tat estimÃ© optimal**.

### Contenu
- PrÃ©sentation des mÃ©thodes **BLUE (Best Linear Unbiased Estimator)** et **3D-VAR**.  
- ImplÃ©mentations sur deux systÃ¨mes physiques de rÃ©fÃ©rence :  
  - un **pendule simple**,  
  - le **systÃ¨me de Lorenz** (attracteur chaotique).  
- Visualisation des trajectoires assimilÃ©es par rapport aux trajectoires vraies et aux observations bruitÃ©es.  
- Illustration du **rÃ´le des matrices de covariance** (erreurs de fond *B* et dâ€™observation *R*) et du **gain optimal K**.

### Objectifs pÃ©dagogiques
- Comprendre les fondements statistiques de lâ€™assimilation de donnÃ©es.  
- Manipuler les Ã©quations de mise Ã  jour de lâ€™Ã©tat et du gain de Kalman.  
- AcquÃ©rir un cadre gÃ©nÃ©ral pour lâ€™intÃ©gration de connaissances physiques dans des modÃ¨les dâ€™apprentissage.

### RÃ©sultats
- Les implÃ©mentations montrent que les algorithmes **BLUE** et **3D-VAR** permettent de corriger efficacement les Ã©carts entre le modÃ¨le et les observations, en ramenant les trajectoires estimÃ©es vers la dynamique rÃ©elle.  
- Ces exercices constituent la base du travail ultÃ©rieur sur les **modÃ¨les de diffusion appliquÃ©s Ã  lâ€™assimilation**.

---

## 2. ModÃ¨les de diffusion â€” `ddpm_nano_completed.ipynb`

Ce notebook constitue une **introduction pratique aux modÃ¨les de diffusion gÃ©nÃ©ratifs**, Ã  partir de lâ€™article fondateur de **Ho, Jain & Abbeel (2020)** :  
> *Denoising Diffusion Probabilistic Models (DDPM)*, arXiv:2006.11239.

### Contenu
- ImplÃ©mentation simplifiÃ©e dâ€™un modÃ¨le DDPM (â€œnano versionâ€) en **PyTorch**.  
- Application sur les bases dâ€™images **MNIST** et **CIFAR-10**.  
- Ã‰tapes dâ€™entraÃ®nement et dâ€™Ã©chantillonnage illustrÃ©es :  
  - *forward diffusion* (ajout progressif de bruit gaussien),  
  - *reverse denoising* (rÃ©tro-processus appris).  
- Visualisation de la diffusion et du dÃ©bruitage Ã  diffÃ©rents pas de temps.  
- Comparaison qualitative avec dâ€™autres modÃ¨les gÃ©nÃ©ratifs (ex. GAN, VAE).

### Objectifs
- Assimiler le fonctionnement des processus de diffusion et leur stabilitÃ© dâ€™apprentissage.  
- Comprendre le rÃ´le du bruit, de la variance et de la prÃ©diction de bruit (*Îµ-prediction*).  
- PrÃ©parer les travaux ultÃ©rieurs sur les **modÃ¨les dâ€™interpolants stochastiques (SDE / ODE)** et leur application Ã  lâ€™assimilation de donnÃ©es.

### RÃ©sultats
- Le modÃ¨le gÃ©nÃ¨re des Ã©chantillons rÃ©alistes Ã  partir de bruit pur.  
- Lâ€™Ã©volution visuelle des Ã©tapes de dÃ©bruitage confirme la bonne convergence du modÃ¨le et la comprÃ©hension du processus de diffusion.  
- Ce notebook sert de **brique expÃ©rimentale de rÃ©fÃ©rence** pour les sections suivantes (inpainting, diffusion sur graphes, interpolants).

---

## 3. Interpolants stochastiques (SDE / ODE)

Cette partie du projet sâ€™appuie sur les travaux rÃ©cents de **M. S. Albergo et E. Vanden-Eijnden** (*Stochastic Interpolants: A Unifying Framework for Flows and Diffusions*, 2023).  
Lâ€™objectif est dâ€™introduire un **cadre continu et unificateur** pour les modÃ¨les gÃ©nÃ©ratifs, en montrant comment les Ã©quations diffÃ©rentielles stochastiques (SDE) et dÃ©terministes (ODE) peuvent relier deux distributions de probabilitÃ© arbitraires au moyen dâ€™un **interpolant stochastique**.

Deux notebooks complÃ©mentaires sont proposÃ©s :

---

### ğŸ”¹ `interpolant_ODE_SDE_2D_notebook.ipynb`

Ce notebook a Ã©tÃ© conÃ§u comme un **support de cours illustrÃ©** pour prÃ©senter les fondements thÃ©oriques et numÃ©riques des interpolants stochastiques.

#### Contenu
- Introduction conceptuelle aux interpolants stochastiques et Ã  leur lien avec les modÃ¨les de diffusion.  
- DÃ©finition formelle de lâ€™interpolant :
  \[
  x_t = I(t, x_0, x_1, z)
  \]
  reliant deux distributions \( \rho_0 \) et \( \rho_1 \) via un bruit latent \( z \).  
- PrÃ©sentation des Ã©quations de **Fokkerâ€“Planck**, du **champ de vitesse \( b_t \)**, du **score \( s_t \)** et du **dÃ©noiseur \( \eta_t \)**.  
- ImplÃ©mentation en PyTorch dâ€™un interpolant 2D entre deux distributions arbitraires :  
  - Distribution initiale \( \rho_0 \) : gaussienne centrÃ©e ;  
  - Distribution cible \( \rho_1 \) : forme sinusoÃ¯dale / en â€œvagueâ€.  
- Estimation des champs \( b \), \( s \) et \( \eta \) via un petit rÃ©seau de neurones entiÃ¨rement connectÃ©.  
- Simulation et visualisation de trajectoires gÃ©nÃ©rÃ©es par ODE et SDE.

#### Objectifs
- Relier les modÃ¨les gÃ©nÃ©ratifs Ã  un cadre probabiliste continu.  
- Comprendre la diffÃ©rence entre une Ã©volution **stochastique (SDE)** et **dÃ©terministe (ODE)**.  
- Visualiser comment les trajectoires stochastiques connectent les deux distributions.

#### RÃ©sultats
- Les interpolants permettent de gÃ©nÃ©rer des points conformes Ã  la distribution cible \( \rho_1 \) Ã  partir de la gaussienne initiale \( \rho_0 \).  
- Les trajectoires obtenues par SDE sont plus diverses, tandis que celles issues de lâ€™ODE sont plus rÃ©guliÃ¨res.  
- Ce notebook illustre de maniÃ¨re pÃ©dagogique la continuitÃ© entre les modÃ¨les de diffusion et les normalizing flows.

---

### ğŸ”¹ `interpolant_ODE_SDE_flowers_64_notebook.ipynb`

AprÃ¨s validation sur des distributions 2D simples, le cadre est Ã©tendu Ã  un **cas visuel rÃ©el** : le **dataset *Oxford Flowers 64Ã—64***.  
Le but est dâ€™observer si les interpolants stochastiques peuvent reproduire des structures visuelles complexes.

#### Contenu
- Chargement du dataset *Oxford Flowers* et prÃ©-traitement des images.  
- ImplÃ©mentation du mÃªme modÃ¨le dâ€™interpolant (SDE/ODE), adaptÃ© Ã  des entrÃ©es haute dimension.  
- EntraÃ®nement sur le **cluster de calcul CLEPS (INRIA)** pendant ~50 Ã©poques.  
- Ã‰chantillonnage de nouvelles images via intÃ©gration des Ã©quations diffÃ©rentielles associÃ©es.  
- Visualisation finale des images gÃ©nÃ©rÃ©es Ã  lâ€™Ã©poque 35 :

![RÃ©sultats sur le dataset Oxford Flowers (epoch 35)](interpolant_ODE_SDE/results/results_epoch_35_flowers.png)
![RÃ©sultats sur le dataset Oxford Flowers (epoch 60)](interpolant_ODE_SDE/results/results_epoch_60_flowers.png)
#### RÃ©sultats
- Les images gÃ©nÃ©rÃ©es sont **visuellement cohÃ©rentes** : formes florales, dÃ©gradÃ©s de couleurs, textures naturelles.  
- Les interpolants reproduisent fidÃ¨lement la diversitÃ© des donnÃ©es, confirmant leur **capacitÃ© de modÃ©lisation continue** entre bruit et structure.  
- La mÃ©thode SDE conserve une plus grande variabilitÃ©, tandis que lâ€™ODE tend vers une reconstruction plus stable mais moins expressive.

#### Apports du travail
- PremiÃ¨re mise en Å“uvre pratique du formalisme â€œStochastic Interpolantsâ€ au sein de lâ€™Ã©quipe ARGO.  
- RÃ©daction dâ€™un **notebook pÃ©dagogique destinÃ© aux Ã©lÃ¨ves de lâ€™Ã‰cole Polytechnique**, visant Ã  introduire les interpolants dans le cadre des modÃ¨les de diffusion.  
- Base de comparaison pour les expÃ©riences ultÃ©rieures dâ€™**assimilation de donnÃ©es** sur images et sur graphes.

---

> Ces deux notebooks forment le cÅ“ur thÃ©orique du stage : ils montrent comment un processus continu dâ€™interpolation probabiliste peut Ãªtre exploitÃ© pour la gÃ©nÃ©ration de donnÃ©es, et ouvrent la voie Ã  leur utilisation dans des contextes dâ€™assimilation.



## 4. Inpainting sur des arbres couvrants uniformes (UST)

Cette partie du projet sâ€™intÃ©resse Ã  lâ€™application des modÃ¨les de diffusion Ã  des **donnÃ©es structurÃ©es de type graphe**, sous forme dâ€™images de **labyrinthes reprÃ©sentant des arbres couvrants uniformes (Uniform Spanning Trees, UST)**.  
Lâ€™objectif est dâ€™examiner si un **modÃ¨le de diffusion conditionnelle** peut **â€œassimilerâ€ des propriÃ©tÃ©s physiques ou structurelles** telles que :
- **la connexitÃ©** (tous les nÅ“uds sont reliÃ©s),  
- **lâ€™absence de cycles**,  
- **le nombre dâ€™arÃªtes fixe** pour un graphe donnÃ©.

Le travail est regroupÃ© dans le dossier `mazes_inpainting_and_utils/`, qui contient les scripts suivants :

| Fichier | Description |
|----------|--------------|
| `inpainting_generating_chosen_mask.py` | Script principal dâ€™inpainting : permet de gÃ©nÃ©rer des masques arbitraires sur les UST et de complÃ©ter les zones manquantes via un modÃ¨le de diffusion. Lâ€™utilisateur peut **dessiner manuellement** le masque Ã  reconstituer. |
| `inpainting_mazes_training_generation_fixed_mask.py` | Variante expÃ©rimentale permettant dâ€™entraÃ®ner le modÃ¨le avec un masque fixe et de comparer les performances selon diffÃ©rents taux de masquage. |
| `utils_*.py` *(selon version)* | Fonctions auxiliaires pour la gÃ©nÃ©ration dâ€™UST (algorithme de Wilson), le traitement dâ€™images et la visualisation. |

---

### Principe

Lâ€™idÃ©e est de considÃ©rer les images dâ€™UST comme des â€œcartesâ€ partielles dâ€™un systÃ¨me Ã  complÃ©ter.  
Une portion du graphe est masquÃ©e, puis le modÃ¨le tente de **reconstruire la partie manquante** en respectant la cohÃ©rence globale.  

Lâ€™approche repose sur :
1. un **modÃ¨le de diffusion conditionnÃ©** sur les rÃ©gions visibles de lâ€™image ;  
2. une **technique de resampling** inspirÃ©e de *RePaint* (Lugmayr et al., 2022),  
   qui permet de rÃ©alimenter en variance les zones reconstruites afin dâ€™amÃ©liorer la continuitÃ© entre parties connues et complÃ©tÃ©es.

---

### Fonctionnement du script

- Le script charge un ensemble dâ€™UST gÃ©nÃ©rÃ©s par lâ€™algorithme de **Wilson**.  
- Lâ€™utilisateur dÃ©finit un **masque** (manuellement ou alÃ©atoirement) reprÃ©sentant les zones Ã  reconstruire.  
- Le modÃ¨le de diffusion rÃ©alise un processus de **bruitage / dÃ©bruitage** sur lâ€™image complÃ¨te, mais ne modifie que les zones masquÃ©es.  
- Le rÃ©sultat est sauvegardÃ© et affichÃ© pour comparaison avec lâ€™image initiale.

---

### Exemples de rÃ©sultats

#### Exemple 1 â€“ Masque manuel
Lâ€™utilisateur sÃ©lectionne un masque de forme libre.  
Le modÃ¨le tente ensuite de combler les zones manquantes.

![Inpainting avec masque manuel](mazes_inpainting_and_utils/results/result_mask_random.png)

#### Exemple 2 â€“ Masque complet
Lorsque le masque couvre toute lâ€™image, le modÃ¨le rÃ©alise une **gÃ©nÃ©ration complÃ¨te** Ã  partir du bruit initial.

![Inpainting sur masque complet](mazes_inpainting_and_utils/results/result_inpainting_full_mask.png)


---

### InterprÃ©tation des rÃ©sultats

Les reconstructions produites montrent une **bonne cohÃ©rence visuelle** avec les images originales :  
les motifs et textures locales sont correctement reproduits, les arÃªtes sâ€™alignent globalement avec la structure attendue.  

Cependant :
- certaines **zones contiennent des cycles** (non dÃ©sirÃ©s) ;  
- la **connexitÃ© nâ€™est pas toujours assurÃ©e**, avec parfois des arÃªtes isolÃ©es ;  
- les propriÃ©tÃ©s topologiques des UST ne sont donc **pas complÃ¨tement apprises** par le modÃ¨le de diffusion.

Ces limites montrent que si le modÃ¨le capture bien les **corrÃ©lations locales**, il nâ€™intÃ¨gre pas encore la **structure globale** imposÃ©e par la thÃ©orie des graphes.  
Une piste dâ€™amÃ©lioration, Ã©voquÃ©e dans le rapport, serait dâ€™intÃ©grer :
- des **contraintes structurelles explicites** (pÃ©nalisation de cycles, connexitÃ©) dans la loss,  
- ou une **diffusion directement sur le graphe** plutÃ´t que sur lâ€™image.

---

### Objectif de cette expÃ©rimentation

Ce travail dâ€™inpainting sert de **pont conceptuel** entre la diffusion sur images et lâ€™assimilation de donnÃ©es physiques :  
comme dans un systÃ¨me rÃ©el oÃ¹ certaines observations sont manquantes, le modÃ¨le doit â€œassimilerâ€ des informations partielles pour **reconstruire un Ã©tat complet cohÃ©rent**.

Il constitue donc une **premiÃ¨re tentative dâ€™assimilation par modÃ¨le de diffusion**, avant les expÃ©riences suivantes sur graphes.

---

> Ces expÃ©riences montrent que les modÃ¨les de diffusion peuvent restituer efficacement la structure apparente dâ€™un systÃ¨me, mais que la prÃ©servation des lois internes (ici les propriÃ©tÃ©s dâ€™un UST) nÃ©cessite des contraintes explicites.  
> Ce constat motivera la derniÃ¨re partie du projet : **la diffusion directement sur graphes**, afin de capturer les relations topologiques sans passer par lâ€™image.


## 5. Diffusion sur graphes â€” GÃ©nÃ©ration dâ€™UST

Ã€ la suite des limites observÃ©es avec la mÃ©thode dâ€™**inpainting par diffusion sur images**, une nouvelle approche a Ã©tÃ© testÃ©e :  
appliquer la **diffusion directement sur les graphes** eux-mÃªmes, plutÃ´t que sur leurs reprÃ©sentations visuelles.  

Lâ€™objectif est de voir si un **modÃ¨le de diffusion discrÃ¨te** peut apprendre Ã  gÃ©nÃ©rer des **arbres couvrants uniformes (UST)** â€” câ€™est-Ã -dire des graphes connexes, acycliques et couvrant tous les nÅ“uds â€” Ã  partir dâ€™une matrice dâ€™adjacence bruitÃ©e.

---

### Structure du dossier `UST_diffusion/`

| Fichier / notebook | RÃ´le |
|---------------------|------|
| `sample_ppgn.ipynb` | Notebook principal : visualisation et Ã©chantillonnage des graphes gÃ©nÃ©rÃ©s par diffusion. |
| `train_ppgn_simple_adj_neigh.py` | Script dâ€™entraÃ®nement du modÃ¨le de diffusion sur matrices dâ€™adjacence. ImplÃ©mente la logique de bruitage / dÃ©bruitage et la loss. |
| `model_ppgn.py` | DÃ©finition du rÃ©seau de neurones principal (**PPGN** â€” Powerful Graph Network), architecture inspirÃ©e des *Graph Neural Networks* (message passing). |
| `graphs.py` | Fonctions utilitaires : gÃ©nÃ©ration de graphes de rÃ©fÃ©rence via lâ€™algorithme de **Wilson**, crÃ©ation de matrices dâ€™adjacence, visualisation et mesure de propriÃ©tÃ©s (connexitÃ©, cycles, etc.). |
| `data/` *(optionnel)* | Contient les datasets de graphes utilisÃ©s pour lâ€™entraÃ®nement et la validation. |

---

### MÃ©thodologie

#### 1. ReprÃ©sentation des donnÃ©es
Chaque graphe est reprÃ©sentÃ© par sa **matrice dâ€™adjacence** \( A \in \{0,1\}^{n \times n} \).  
Les graphes dâ€™entraÃ®nement sont des **UST gÃ©nÃ©rÃ©s par lâ€™algorithme de Wilson**.  

#### 2. Processus de diffusion
Un **bruit discret** est ajoutÃ© sur les arÃªtes :
- Ã  chaque Ã©tape, certaines arÃªtes sont ajoutÃ©es ou supprimÃ©es avec une probabilitÃ© donnÃ©e ;
- le modÃ¨le apprend Ã  prÃ©dire le bruit ajoutÃ© entre \( A_t \) (bruitÃ©) et \( A_{t-1} \) (Ã©tat prÃ©cÃ©dent).

#### 3. RÃ©seau utilisÃ©
Le modÃ¨le est basÃ© sur une architecture **PPGN (Powerful Graph Network)**, adaptÃ©e Ã  la manipulation de matrices dâ€™adjacence :  
- elle encode les relations entre nÅ“uds via un mÃ©canisme de message passing ;  
- elle prÃ©serve la permutation-invariance ;  
- elle permet dâ€™apprendre des reprÃ©sentations locales et globales du graphe.

#### 4. Fonction de perte
La loss intÃ¨gre plusieurs termes :
- **erreur de reconstruction** (prÃ©diction du bruit) ;
- **pÃ©nalitÃ©s structurelles** :
  - nombre de cycles,
  - connexitÃ© du graphe,
  - nombre total dâ€™arÃªtes (fixÃ© Ã  \( n-1 \) pour un UST),
  - nÅ“uds isolÃ©s.  

Chaque pÃ©nalisation est pondÃ©rÃ©e par un hyperparamÃ¨tre, ajustÃ© expÃ©rimentalement.

---

### ExpÃ©riences

Les graphes ont Ã©tÃ© gÃ©nÃ©rÃ©s et entraÃ®nÃ©s pour diffÃ©rentes tailles de grilles (de \(4\times4\) Ã  \(10\times10\)).  
Les figures ci-dessous illustrent le **dÃ©bruitage progressif** dâ€™un graphe vers une structure proche dâ€™un UST.

| Ã‰tape | Description | Observation |
|-------|--------------|-------------|
| 1 | Graphe bruitÃ© (arÃªtes alÃ©atoires) | Nombreux cycles et composantes isolÃ©es |
| 2 | Diffusion discrÃ¨te sur 64 Ã©tapes | Suppression progressive des cycles |
| 3 | Graphe reconstruit | Structure proche dâ€™un arbre couvrant |

Ã€ la fin du processus, les graphes produits prÃ©sentent souvent **moins de cycles** et une **meilleure connexitÃ©** que les graphes alÃ©atoires de dÃ©part.

---

### Ã‰valuation quantitative

Pour quantifier la qualitÃ© des graphes gÃ©nÃ©rÃ©s :
- On applique un **post-traitement** supprimant les cycles restants et connectant les composantes isolÃ©es.
- On compte le **nombre moyen de modifications nÃ©cessaires** pour transformer le graphe gÃ©nÃ©rÃ© en un vÃ©ritable UST.

| Taille du graphe | Changements moyens (graphes alÃ©atoires) | Changements moyens (graphes gÃ©nÃ©rÃ©s par diffusion) |
|------------------:|----------------------------------------:|----------------------------------------------------:|
| 16 nÅ“uds | 7.3 | **2.5** |
| 36 nÅ“uds | 13.8 | **7.1** |
| 64 nÅ“uds | 21.4 | **11.3** |

Les graphes issus du modÃ¨le nÃ©cessitent environ **deux fois moins de corrections** que des graphes purement alÃ©atoires, ce qui montre que la diffusion apprend partiellement les propriÃ©tÃ©s structurelles recherchÃ©es.

---

### InterprÃ©tation et perspectives

- Le modÃ¨le de diffusion **assimile partiellement les lois structurelles** des arbres couvrants : les graphes gÃ©nÃ©rÃ©s sont souvent proches dâ€™UST, surtout pour de petites tailles.  
- Pour les graphes plus grands, les performances se dÃ©gradent Ã  cause :
  - de la **sparsitÃ©** croissante des matrices dâ€™adjacence,  
  - dâ€™un **manque de capacitÃ©** du modÃ¨le,  
  - et de la **difficultÃ© de rÃ©gularisation** des contraintes topologiques.

Des pistes dâ€™amÃ©lioration proposÃ©es :
- reprÃ©senter les graphes sous forme de **vecteurs dâ€™arÃªtes autorisÃ©es** plutÃ´t que matrices carrÃ©es ;
- introduire une **pÃ©nalisation adaptative** (cycles / connexitÃ©) au cours de la gÃ©nÃ©ration ;
- combiner diffusion discrÃ¨te et apprentissage par renforcement pour contraindre la topologie en ligne.

---

### Bilan de cette derniÃ¨re Ã©tape

Cette expÃ©rimentation conclut le stage en montrant la **faisabilitÃ© dâ€™une diffusion sur graphes** :  
mÃªme si les modÃ¨les ne captent pas encore toutes les propriÃ©tÃ©s physiques, ils constituent une premiÃ¨re base pour une **assimilation de donnÃ©es topologique**.  

> En somme, la transition de lâ€™inpainting dâ€™images vers la diffusion sur graphes reprÃ©sente un changement dâ€™Ã©chelle conceptuel :  
> passer dâ€™une assimilation â€œvisuelleâ€ Ã  une assimilation â€œstructurelleâ€, plus proche des systÃ¨mes physiques modÃ©lisÃ©s par graphes.


